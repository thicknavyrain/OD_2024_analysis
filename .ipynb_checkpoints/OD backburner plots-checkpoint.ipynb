{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d7db831-6d3d-402b-a0ff-49648153055e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Define vehicle categories\n",
    "vehicle_categories = ['car', 'trotro', 'truck', 'motorcycle', 'lorry', 'bus', 'taxi', 'van', 'bicycle']\n",
    "\n",
    "# Define grid layout\n",
    "nrows, ncols = 4, 3\n",
    "fig, axes = plt.subplots(nrows, ncols, figsize=(15, 15))\n",
    "fig.suptitle('Vehicle Proportions by Hour by Site', fontsize=16)\n",
    "\n",
    "# Flatten the axes array for easy access\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Define a color for each vehicle category\n",
    "colors = [plt.cm.viridis(i/len(vehicle_categories)) for i in range(len(vehicle_categories))]\n",
    "\n",
    "# Plot data for each site\n",
    "for i, (site, site_details) in enumerate(site_type.items()):\n",
    "    ax = axes[i] if i < (nrows * ncols - 1) else axes[-1]\n",
    "    \n",
    "    # Prepare data for plotting\n",
    "    site_data = hourly_averages[hourly_averages['site_id'] == site]\n",
    "    site_data = site_data.set_index('datetime')\n",
    "    \n",
    "    # Filter columns that match vehicle categories\n",
    "    vehicle_data_by_hour = site_data[[cat+\"_counts\" for cat in site_data.columns if cat in vehicle_categories]]\n",
    "    \n",
    "    # Group by the hour and calculate the mean for each vehicle category\n",
    "    vehicle_data_by_hour = vehicle_data_by_hour.groupby(site_data.index.hour).mean()\n",
    "    \n",
    "    # Normalize the data to sum to 1 across vehicle categories for each hour\n",
    "    proportions = vehicle_data_by_hour.div(vehicle_data_by_hour.sum(axis=1), axis=0)\n",
    "    \n",
    "    # Initialize bar_bottom for each site\n",
    "    bar_bottom = np.zeros(24)\n",
    "    \n",
    "    # Plot stacked bars for each vehicle category\n",
    "    for idx, vehicle_cat in enumerate(vehicle_categories):\n",
    "        # Select the data for the current vehicle category and fill NaNs with zero for missing hours\n",
    "        counts = proportions[vehicle_cat+\"_counts\"].reindex(range(24)).fillna(0).values\n",
    "        \n",
    "        # Stack the bar for the current vehicle category\n",
    "        ax.bar(range(24), counts, bottom=bar_bottom, color=colors[idx], label=vehicle_cat, width=1)\n",
    "        \n",
    "        # Update bar_bottom for the next category\n",
    "        bar_bottom += counts\n",
    "    \n",
    "    ax.set_title(site_details[0])\n",
    "    ax.set_xlim(0, 23)\n",
    "    ax.set_xticks(range(24))\n",
    "    ax.set_xticklabels([f'{hour}:00' for hour in range(24)], rotation=90)\n",
    "    ax.grid(True, which='major', axis='x', linestyle='-', linewidth=0.5)\n",
    "    ax.grid(True, which='major', axis='y', linestyle='-', linewidth=0.5)\n",
    "    \n",
    "    if i >= nrows * (ncols - 1):  # These plots will be on the bottom and should display x-ticks\n",
    "        ax.tick_params(labelbottom=True)\n",
    "    else:\n",
    "        ax.tick_params(labelbottom=False)\n",
    "\n",
    "# Remove empty subplots\n",
    "for j in range(i + 1, nrows * ncols):\n",
    "    fig.delaxes(axes[j])\n",
    "\n",
    "# Create a single legend at the bottom of the figure\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "fig.legend(handles, labels, loc='lower center', ncol=len(vehicle_categories)//2 + 1, bbox_to_anchor=(0.5, -0.02), fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(top=0.92, bottom=0.15)  # Adjust bottom to make space for the legend\n",
    "\n",
    "plt.savefig(f'./results/diurnal/vehicle_proportions.png')\n",
    "# plt.close(fig)  # Close the figure to avoid display issues\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da12445-3502-49eb-b1b3-35cbe90c14d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Create a new DataFrame to avoid SettingWithCopyWarning\n",
    "object_data = fixed_object_data.copy()\n",
    "\n",
    "# Add a column for the week of the year and the year\n",
    "object_data['week_of_year'] = object_data['datetime'].dt.isocalendar().week\n",
    "object_data['year'] = object_data['datetime'].dt.isocalendar().year\n",
    "\n",
    "# Step 2: Sum the counts within each week for each camera at each site\n",
    "weekly_counts = object_data.groupby(['site_id', 'camera_name', 'year', 'week_of_year'])[count_cols].sum().reset_index()\n",
    "\n",
    "# Step 3: Group by 'site_id', 'year', and 'week_of_year', then calculate the mean for each object category\n",
    "weekly_averages = weekly_counts.groupby(['site_id', 'year', 'week_of_year'])[count_cols].mean().reset_index()\n",
    "\n",
    "# Step 4: Calculate mean and standard deviation for each site and year\n",
    "mean_std_by_site_year = weekly_averages.groupby(['site_id', 'year'])[count_cols].agg(['mean', 'std'])\n",
    "\n",
    "# Flatten the multi-index in columns\n",
    "mean_std_by_site_year.columns = ['_'.join(col).strip() for col in mean_std_by_site_year.columns.values]\n",
    "mean_std_by_site_year.reset_index(inplace=True)\n",
    "\n",
    "# Define grid layout\n",
    "nrows, ncols = 4, 3\n",
    "\n",
    "# Create the plots for each super-category\n",
    "for super_cat, categories in super_categories.items():\n",
    "    # Correct column references\n",
    "    count_columns = [cat + '_counts' for cat in categories]\n",
    "\n",
    "    # Initialize the plot for this super-category\n",
    "    fig, axes = plt.subplots(nrows, ncols, figsize=(15, 15))\n",
    "    fig.suptitle(super_cat.capitalize().replace(\"_\", \" \") + ' - Weekly Trend by Site', fontsize=16)\n",
    "\n",
    "    # Flatten the axes array for easy access and plotting\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    # Remove the subplot that will be empty (bottom left)\n",
    "    fig.delaxes(axes[-3])\n",
    "    axes[-3] = None\n",
    "\n",
    "    # Plot data for each site\n",
    "    for i, (site, site_details) in enumerate(site_type.items()):\n",
    "        ax = axes[i] if i < 9 else axes[10]  # Use existing axes for the 10th plot\n",
    "    \n",
    "        # Prepare data for plotting\n",
    "        site_weekly_data = weekly_averages[weekly_averages['site_id'] == site]\n",
    "        \n",
    "        # Plot a z-normalized line for each year's data\n",
    "        for year in site_weekly_data['year'].unique():\n",
    "            year_data = site_weekly_data[site_weekly_data['year'] == year]\n",
    "            mean_values = mean_std_by_site_year.loc[(mean_std_by_site_year['site_id'] == site) & \n",
    "                                                    (mean_std_by_site_year['year'] == year), \n",
    "                                                    [col + '_mean' for col in count_columns]].values.squeeze()\n",
    "            std_values = mean_std_by_site_year.loc[(mean_std_by_site_year['site_id'] == site) & \n",
    "                                                   (mean_std_by_site_year['year'] == year), \n",
    "                                                   [col + '_std' for col in count_columns]].values.squeeze()\n",
    "            \n",
    "            # Normalize each week's counts and plot\n",
    "            z_scores = (year_data[count_columns] - mean_values) / std_values\n",
    "            ax.plot(year_data['week_of_year'], z_scores.mean(axis=1), label=f'{year}')\n",
    "\n",
    "        ax.set_title(site_details[0])\n",
    "        ax.set_xlim(1, 52)\n",
    "        ax.legend()\n",
    "\n",
    "        # Draw grid lines for every week\n",
    "        ax.grid(True, which='major', axis='x', linestyle='-', linewidth=0.5)\n",
    "        ax.grid(True, which='major', axis='y', linestyle='-', linewidth=0.5)\n",
    "\n",
    "        # Manage x-ticks visibility for specified subplots\n",
    "        if i in [6, 8, 9]:  # Indices for subplots that should show x-ticks\n",
    "            ax.set_xticks(np.arange(1, 53, 1))  # Adjusted for weekly ticks\n",
    "            ax.tick_params(labelbottom=True)\n",
    "        else:\n",
    "            ax.set_xticks(np.arange(1, 53, 1))  # Adjusted for weekly ticks\n",
    "            ax.tick_params(labelbottom=False)\n",
    "\n",
    "    # Remove the last (empty) subplot from the grid\n",
    "    fig.delaxes(axes[-1])\n",
    "    axes[-1] = None\n",
    "\n",
    "    # Adjust layout and save the figure\n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(top=0.92)\n",
    "    plt.savefig(f'./results/{super_cat}_weekly_znormalized.png')\n",
    "    # plt.close(fig)  # Close the figure to avoid display issues\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb69292e-94dc-4507-a997-8091b8512ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Create a new DataFrame to avoid SettingWithCopyWarning\n",
    "object_data = fixed_object_data.copy()\n",
    "\n",
    "# Add a column for the week of the year and the year\n",
    "object_data['week_of_year'] = object_data['datetime'].dt.isocalendar().week\n",
    "object_data['year'] = object_data['datetime'].dt.isocalendar().year\n",
    "\n",
    "# Sum counts for each supercategory\n",
    "for super_cat, categories in super_categories.items():\n",
    "    object_data[super_cat + '_counts'] = object_data[[cat + '_counts' for cat in categories]].sum(axis=1)\n",
    "\n",
    "# Add a column for the month of the year\n",
    "object_data['month_of_year'] = object_data['datetime'].dt.month\n",
    "\n",
    "# Step 2: Group and sum the counts within each month for each site\n",
    "monthly_counts = object_data.groupby(['site_id', 'year', 'month_of_year']).agg(\n",
    "    {super_cat + '_counts': 'sum' for super_cat in super_categories.keys()}\n",
    ").reset_index()\n",
    "\n",
    "# Calculate total counts per supercategory for each site across all time\n",
    "total_counts_per_site = monthly_counts.groupby(['site_id']).agg(\n",
    "    {super_cat + '_counts': 'sum' for super_cat in super_categories.keys()}\n",
    ").reset_index()\n",
    "\n",
    "# Merge the monthly averages with the total counts to get the share\n",
    "monthly_shares = monthly_counts.merge(total_counts_per_site, on='site_id', suffixes=('', '_total'))\n",
    "\n",
    "# Calculate the monthly share for each supercategory\n",
    "for super_cat in super_categories.keys():\n",
    "    monthly_shares[super_cat + '_share'] = monthly_shares[super_cat + '_counts'] / monthly_shares[super_cat + '_counts_total']\n",
    "\n",
    "# Define grid layout for plotting\n",
    "nrows, ncols = 4, 3\n",
    "\n",
    "# Plotting\n",
    "for super_cat in super_categories.keys():\n",
    "    fig, axes = plt.subplots(nrows, ncols, figsize=(15, 15))\n",
    "    fig.suptitle(f'{super_cat.capitalize().replace(\"_\", \" \")} - Monthly Share by Site', fontsize=16)\n",
    "\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for i, (site, site_details) in enumerate(site_type.items()):\n",
    "        ax = axes[i] if i < (nrows * ncols - 1) else axes[-1]\n",
    "\n",
    "        # Initialize an empty list to collect all max values across years\n",
    "        all_max_values = []\n",
    "\n",
    "        for year in monthly_shares['year'].unique():\n",
    "            yearly_data = monthly_shares[(monthly_shares['site_id'] == site) & (monthly_shares['year'] == year)]\n",
    "\n",
    "            if not yearly_data[super_cat + '_share'].isnull().all():  # Check if all values are NaN\n",
    "                ax.plot(yearly_data['month_of_year'], yearly_data[super_cat + '_share'], label=f'Year {year}')\n",
    "                # Collect all non-NaN max values\n",
    "                max_value = yearly_data[super_cat + '_share'].max()\n",
    "                if pd.notnull(max_value):\n",
    "                    all_max_values.append(max_value)\n",
    "\n",
    "        # Set y-limit based on the max value found across all years\n",
    "        if all_max_values:  # Check if the list is not empty\n",
    "            ax.set_ylim(0, max(all_max_values) * 1.1)\n",
    "\n",
    "        ax.set_title(f'{site} - {super_cat}')\n",
    "        ax.set_xlim(1, 12)\n",
    "        ax.set_xticks(range(1, 13))\n",
    "        # When setting x-tick labels\n",
    "        ax.set_xticklabels([calendar.month_abbr[m] for m in range(1, 13)])\n",
    "        ax.legend()\n",
    "        ax.grid(True)\n",
    "\n",
    "        if i >= nrows * (ncols - 1):  # These plots will be on the bottom and should display x-ticks\n",
    "            ax.tick_params(labelbottom=True)\n",
    "        else:\n",
    "            ax.tick_params(labelbottom=False)\n",
    "\n",
    "    for j in range(i + 1, nrows * ncols):\n",
    "        fig.delaxes(axes[j])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(top=0.92)\n",
    "    plt.savefig(f'./results/monthly_plots/{super_cat}_monthly_share.png')\n",
    "    # plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae22bba-e5e8-4016-8e36-23284b57dc13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Step 1: Create a new DataFrame to avoid SettingWithCopyWarning\n",
    "object_data = fixed_object_data.copy()\n",
    "\n",
    "# Add a column for the week of the year and the year\n",
    "object_data['week_of_year'] = object_data['datetime'].dt.isocalendar().week\n",
    "object_data['year'] = object_data['datetime'].dt.isocalendar().year\n",
    "\n",
    "# Sum counts for each supercategory\n",
    "for super_cat, categories in super_categories.items():\n",
    "    object_data[super_cat + '_counts'] = object_data[[cat + '_counts' for cat in categories]].sum(axis=1)\n",
    "\n",
    "# Step 2: Group and sum the counts within each week for each camera at each site\n",
    "weekly_counts = object_data.groupby(['site_id', 'year', 'week_of_year']).agg(\n",
    "    {super_cat + '_counts': 'sum' for super_cat in super_categories.keys()}\n",
    ").reset_index()\n",
    "\n",
    "# Calculate total counts per supercategory for each site and year\n",
    "total_counts_per_year_site = weekly_counts.groupby(['site_id', 'year']).agg(\n",
    "    {super_cat + '_counts': 'sum' for super_cat in super_categories.keys()}\n",
    ").reset_index()\n",
    "\n",
    "# Calculate the weekly share for each supercategory as a portion of the total per year, per site\n",
    "for super_cat in super_categories.keys():\n",
    "    # Join with the yearly totals\n",
    "    weekly_counts = weekly_counts.merge(\n",
    "        total_counts_per_year_site[['site_id', 'year', super_cat + '_counts']],\n",
    "        on=['site_id', 'year'],\n",
    "        suffixes=('', '_yearly_total')\n",
    "    )\n",
    "    \n",
    "    # Calculate the proportion\n",
    "    weekly_counts[super_cat + '_share'] = weekly_counts[super_cat + '_counts'] / weekly_counts[super_cat + '_counts_yearly_total']\n",
    "\n",
    "# Filter out weeks without complete data\n",
    "weekly_counts = weekly_counts.dropna(subset=[super_cat + '_share' for super_cat in super_categories.keys()])\n",
    "\n",
    "# Define grid layout for plotting\n",
    "nrows, ncols = 4, 3\n",
    "\n",
    "# Plotting\n",
    "for super_cat in super_categories.keys():\n",
    "    fig, axes = plt.subplots(nrows, ncols, figsize=(15, 15))\n",
    "    fig.suptitle(f'{super_cat.capitalize().replace(\"_\", \" \")} - Weekly Share by Site and Year', fontsize=16)\n",
    "\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for i, (site, site_details) in enumerate(site_type.items()):\n",
    "        ax = axes[i] if i < (nrows * ncols - 1) else axes[-1]\n",
    "        \n",
    "        for year in weekly_counts['year'].unique():\n",
    "            yearly_data = weekly_counts[(weekly_counts['site_id'] == site) & (weekly_counts['year'] == year)]\n",
    "            if not yearly_data.empty:  # Check if there is data for the year\n",
    "                ax.plot(yearly_data['week_of_year'], yearly_data[super_cat + '_share'], label=f'Year {year}')\n",
    "\n",
    "        ax.set_title(f'{site} - {super_cat}')\n",
    "        ax.set_xlim(1, 53)\n",
    "        # ax.set_ylim(0, weekly_counts[super_cat + '_share'].max() * 1.1)  # Adjust the limit based on the max share\n",
    "        ax.set_ylim(0, 0.1)  # Adjust the limit based on the max share\n",
    "        ax.legend()\n",
    "        ax.grid(True)\n",
    "\n",
    "        if i >= nrows * (ncols - 1):  # These plots will be on the bottom and should display x-ticks\n",
    "            ax.tick_params(labelbottom=True)\n",
    "        else:\n",
    "            ax.tick_params(labelbottom=False)\n",
    "\n",
    "    for j in range(i + 1, nrows * ncols):\n",
    "        fig.delaxes(axes[j])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(top=0.92)\n",
    "    plt.savefig(f'./results/{super_cat}_weekly_share_by_year.png')\n",
    "    # plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35998ce8-0705-4a4f-a8ab-abdfa383bcb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "# Define the correlation categories\n",
    "corr_categories = ['people', 'two_wheelers', 'small_vehicles', 'large_vehicles', 'market', 'refuse', 'animal']\n",
    "\n",
    "# Concatenate data from all sites and cameras\n",
    "multi_frame = []\n",
    "for site in site_cams:\n",
    "    for cam in site_cams[site]:\n",
    "        multi_frame.append(fixed_object_data.loc[(fixed_object_data['site_id'] == site) & (fixed_object_data['camera'] == cam), [col + '_counts' for col in corr_categories]])\n",
    "\n",
    "corr_frame = pd.concat(multi_frame)\n",
    "\n",
    "# Rename columns to remove '_counts'\n",
    "corr_frame.columns = [col.replace('_counts', '') for col in corr_frame.columns]\n",
    "\n",
    "# Calculate correlation coefficients\n",
    "method = 'pearson'\n",
    "corr_matrix = corr_frame.astype(float).corr(method=method)\n",
    "\n",
    "# Calculate p-values\n",
    "p_matrix = pd.DataFrame(columns=corr_categories, index=corr_categories)\n",
    "for i in range(len(corr_categories)):\n",
    "    for j in range(i, len(corr_categories)):\n",
    "        corr, pval = pearsonr(corr_frame[corr_categories[i]], corr_frame[corr_categories[j]])\n",
    "        p_matrix.iloc[i, j] = pval\n",
    "        p_matrix.iloc[j, i] = pval\n",
    "\n",
    "# Create labels with correlation coefficients and p-values\n",
    "labels = np.asarray([\"{:.3f}\\n({:.5f})\".format(corr, pval) for corr, pval in zip(corr_matrix.values.flatten(), p_matrix.values.flatten())]).reshape(len(corr_categories), len(corr_categories))\n",
    "\n",
    "# Replace p-values of 0.00000 with <0.00001\n",
    "labels = np.where(labels == \"(0.00000)\", \"(<0.00001)\", labels)\n",
    "\n",
    "# Create a mask for the upper triangle\n",
    "mask = np.triu(np.ones_like(corr_matrix, dtype=bool), k=1)\n",
    "\n",
    "# Create the heatmap\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "cmap = 'seismic'\n",
    "corr_plot = sns.heatmap(corr_matrix, annot=labels, fmt='', cmap=plt.get_cmap(cmap), vmin=-1, vmax=1, cbar=False, ax=ax, mask=mask)\n",
    "\n",
    "# Set x and y labels\n",
    "ax.set_xticklabels([obj.replace('market', 'market-related').capitalize().replace(\"_\", \" \") for obj in corr_categories], rotation=\"vertical\")\n",
    "ax.set_yticklabels([obj.replace('market', 'market-related').capitalize().replace(\"_\", \" \") for obj in corr_categories], rotation=\"horizontal\")\n",
    "\n",
    "# Adjust the y-axis limits to make the plot square\n",
    "b, t = plt.ylim()\n",
    "b += 0.5\n",
    "t -= 0.5\n",
    "plt.ylim(b, t)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Save the plot\n",
    "fig_type = \"images_supercat_final\"\n",
    "corr_plot.get_figure().savefig(f'results/{fig_type}_corr_{method}.png', format='png', bbox_inches='tight', pad_inches=0.0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
